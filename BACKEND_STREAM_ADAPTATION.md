# åç«¯æµå¼è¾“å‡ºé€‚é…å®Œæˆ

## âœ… å·²å®Œæˆçš„æ”¹é€ 

### 1. **LLMæœåŠ¡å±‚ (llm_service.py)**

#### ä¿®æ”¹ `call_model` æ–¹æ³•
- æ›´æ–°è¿”å›ç±»å‹ï¼šstream=True æ—¶è¿”å› AsyncGenerator
- æ·»åŠ æµå¼æ¨¡å¼æ—¥å¿—è®°å½•

```python
async def call_model(
    model_config: ModelConfigDB,
    prompt: str,
    system_prompt: Optional[str] = None,
    params: Optional[Dict[str, Any]] = None,
    stream: bool = False,  # æµå¼å‚æ•°
    tools: Optional[List[Dict[str, Any]]] = None
):
    """
    è°ƒç”¨å¤§æ¨¡å‹
    
    Returns:
        å¦‚æœstream=Trueï¼Œè¿”å›AsyncGenerator
        å¦‚æœstream=Falseï¼Œè¿”å›åŒ…å«outputã€metricsçš„å­—å…¸
    """
```

#### æ–°å¢ `_stream_openai_response` æ–¹æ³•
æµå¼å“åº”å¤„ç†å™¨ï¼Œæ”¯æŒï¼š
- é€å—è¯»å–OpenAIæµå¼å“åº”
- å®æ—¶yieldæ–‡æœ¬å†…å®¹
- å¤„ç†å·¥å…·è°ƒç”¨
- æ”¶é›†tokenä½¿ç”¨æƒ…å†µå’Œæ€§èƒ½æŒ‡æ ‡
- å‘é€å®Œæˆä¿¡å·å’Œæœ€ç»ˆå“åº”

```python
async def _stream_openai_response(
    client: AsyncOpenAI,
    request_params: Dict[str, Any],
    start_time: float,
    model_config: ModelConfigDB
) -> AsyncGenerator[Dict[str, Any], None]:
    """
    å¤„ç†OpenAIæµå¼å“åº”
    
    Yields:
        åŒ…å«æµå¼æ•°æ®å—çš„å­—å…¸ï¼š
        - {"content": "æ–‡æœ¬", "done": False}  # æ–‡æœ¬å—
        - {"tool_call": {...}, "done": False}  # å·¥å…·è°ƒç”¨
        - {"done": True, "final_response": {...}, "metrics": {...}}  # å®Œæˆ
    """
```

#### ä¿®æ”¹ `_call_openai` æ–¹æ³•
```python
# å¦‚æœæ˜¯æµå¼æ¨¡å¼ï¼Œè¿”å›å¼‚æ­¥ç”Ÿæˆå™¨
if stream:
    return LLMService._stream_openai_response(
        client, request_params, start_time, model_config
    )
```

### 2. **è°ƒè¯•API (debug.py)**

#### ä¿®æ”¹ `ChatRequest` æ¨¡å‹
æ·»åŠ  `stream` å‚æ•°ï¼š

```python
class ChatRequest(BaseModel):
    model_id: int
    prompt: str
    system_prompt: Optional[str] = None
    params: Optional[Dict[str, Any]] = None
    tool_ids: Optional[list[int]] = None
    stream: bool = Field(False, description="æ˜¯å¦ä½¿ç”¨æµå¼è¾“å‡º")  # æ–°å¢
```

#### ä¿®æ”¹ `/chat` æ¥å£
æ”¯æŒæ ¹æ® `stream` å‚æ•°è¿”å›ä¸åŒå“åº”ï¼š

```python
@router.post("/chat")
async def debug_chat(request: ChatRequest, db: Session = Depends(get_db)):
    """å•æ¬¡å¯¹è¯æµ‹è¯•ï¼Œæ”¯æŒæµå¼å’Œéæµå¼"""
    
    # å¦‚æœæ˜¯æµå¼æ¨¡å¼ï¼Œè¿”å› StreamingResponse
    if request.stream:
        async def event_generator():
            stream_result = await LLMService.call_model(..., stream=True)
            
            # é€å—å‘é€æµå¼æ•°æ®
            async for chunk in stream_result:
                yield f"data: {json.dumps(chunk)}\n\n"
            
            # å‘é€ç»“æŸæ ‡è®°
            yield "data: [DONE]\n\n"
        
        return StreamingResponse(
            event_generator(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "X-Accel-Buffering": "no"
            }
        )
    
    # éæµå¼æ¨¡å¼
    result = await LLMService.call_model(..., stream=False)
    return ChatResponse(...)
```

## ğŸ“Š æ•°æ®æµç¨‹

### æµå¼è¾“å‡ºæµç¨‹

```
å‰ç«¯è¯·æ±‚ (stream=true)
    â†“
FastAPI debug.chat æ¥å£
    â†“
LLMService.call_model(stream=True)
    â†“
_stream_openai_response() ç”Ÿæˆå™¨
    â†“
OpenAI API æµå¼å“åº”
    â†“
é€å— yield æ•°æ®
    â†“
StreamingResponse (SSEæ ¼å¼)
    â†“
å‰ç«¯é€å—æ¥æ”¶å’Œæ˜¾ç¤º
```

### SSE æ•°æ®æ ¼å¼

```
data: {"content": "è¿™", "done": false}
data: {"content": "æ˜¯", "done": false}
data: {"content": "æµ‹è¯•", "done": false}
data: {"done": true, "final_response": {...}, "metrics": {...}}
data: [DONE]
```

## ğŸ¯ æ”¯æŒçš„åŠŸèƒ½

### âœ… å·²å®ç°
- [x] æ–‡æœ¬å†…å®¹æµå¼è¾“å‡º
- [x] Tokenç»Ÿè®¡å’Œæ€§èƒ½æŒ‡æ ‡
- [x] æˆæœ¬ä¼°ç®—
- [x] é”™è¯¯å¤„ç†
- [x] å®Œæˆä¿¡å·
- [x] æœ€ç»ˆå“åº”æ±‡æ€»

### â³ å¾…æ‰©å±•
- [ ] å·¥å…·è°ƒç”¨çš„æµå¼å¤„ç†ï¼ˆtool_callsåœ¨æµå¼æ¨¡å¼ä¸‹çš„å±•ç¤ºï¼‰
- [ ] Anthropic Claude æµå¼æ”¯æŒ
- [ ] æœ¬åœ°æ¨¡å‹æµå¼æ”¯æŒ
- [ ] è‡ªå®šä¹‰provideræµå¼æ”¯æŒ

## ğŸ§ª æµ‹è¯•æ–¹æ³•

### æ–¹æ³•1ï¼šä½¿ç”¨è°ƒè¯•é¡µé¢UI
1. æ‰“å¼€: http://127.0.0.1:8000/pages/debug.html
2. é€‰æ‹©æ¨¡å‹
3. å¼€å¯"æµå¼è¾“å‡º"å¼€å…³
4. è¾“å…¥æç¤ºè¯
5. ç‚¹å‡»å‘é€
6. è§‚å¯Ÿå®æ—¶è¾“å‡º

### æ–¹æ³•2ï¼šä½¿ç”¨curlæµ‹è¯•

**æµå¼æ¨¡å¼ï¼š**
```bash
curl -N -X POST http://127.0.0.1:8000/api/debug/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model_id": 1,
    "prompt": "å†™ä¸€é¦–å…³äºäººå·¥æ™ºèƒ½çš„è¯—",
    "stream": true
  }'
```

**æ™®é€šæ¨¡å¼ï¼š**
```bash
curl -X POST http://127.0.0.1:8000/api/debug/chat \
  -H "Content-Type: application/json" \
  -d '{
    "model_id": 1,
    "prompt": "ä½ å¥½",
    "stream": false
  }'
```

### æ–¹æ³•3ï¼šPythonæµ‹è¯•è„šæœ¬

```python
import httpx
import json

async def test_stream():
    async with httpx.AsyncClient() as client:
        async with client.stream(
            'POST',
            'http://127.0.0.1:8000/api/debug/chat',
            json={
                "model_id": 1,
                "prompt": "ç»™æˆ‘è®²ä¸ªç¬‘è¯",
                "stream": True
            },
            timeout=60.0
        ) as response:
            async for line in response.aiter_lines():
                if line.startswith('data: '):
                    data = line[6:]
                    if data == '[DONE]':
                        break
                    chunk = json.loads(data)
                    if 'content' in chunk:
                        print(chunk['content'], end='', flush=True)
            print()

# è¿è¡Œ
import asyncio
asyncio.run(test_stream())
```

## ğŸ“ æ—¥å¿—ç¤ºä¾‹

### æµå¼è¾“å‡ºæ—¥å¿—

```
2025-10-27 10:57:00,000 - app.services.llm_service - INFO - ================
2025-10-27 10:57:00,000 - app.services.llm_service - INFO - ğŸš€ å¼€å§‹è°ƒç”¨å¤§æ¨¡å‹
2025-10-27 10:57:00,000 - app.services.llm_service - INFO - æ¨¡å‹é…ç½®: deepseek-ai/DeepSeek-V3
2025-10-27 10:57:00,000 - app.services.llm_service - INFO - æµå¼æ¨¡å¼: True
2025-10-27 10:57:00,000 - app.services.llm_service - INFO - ğŸ“¡ è°ƒç”¨ OpenAI API
2025-10-27 10:57:00,000 - app.services.llm_service - INFO - ğŸ”„ å¼€å§‹æµå¼è¾“å‡º
2025-10-27 10:57:05,000 - app.services.llm_service - INFO - âœ… æµå¼è¾“å‡ºå®Œæˆ
2025-10-27 10:57:05,000 - app.services.llm_service - INFO - ğŸ“Š å“åº”æ—¶é—´: 5.00s
2025-10-27 10:57:05,000 - app.services.llm_service - INFO - ğŸ“Š Tokens: 10 + 150 = 160
2025-10-27 10:57:05,000 - app.services.llm_service - INFO - ğŸ“ å®Œæ•´è¾“å‡ºé•¿åº¦: 456 å­—ç¬¦
```

## âš ï¸ æ³¨æ„äº‹é¡¹

### 1. **è¶…æ—¶è®¾ç½®**
æµå¼è¯·æ±‚å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œå»ºè®®è®¾ç½®åˆç†çš„è¶…æ—¶ï¼š
```python
timeout=httpx.Timeout(60.0, connect=5.0)
```

### 2. **ç¼“å†²é—®é¢˜**
æŸäº›ä»£ç†å¯èƒ½ä¼šç¼“å†²SSEå“åº”ï¼Œç¡®ä¿è®¾ç½®æ­£ç¡®çš„å“åº”å¤´ï¼š
```python
headers={
    "Cache-Control": "no-cache",
    "X-Accel-Buffering": "no"  # Nginx
}
```

### 3. **é”™è¯¯å¤„ç†**
æµå¼è¾“å‡ºè¿‡ç¨‹ä¸­çš„é”™è¯¯ä¼šé€šè¿‡ç‰¹æ®Šçš„æ•°æ®åŒ…å‘é€ï¼š
```json
{"done": true, "error": "é”™è¯¯ä¿¡æ¯", "status": "error"}
```

### 4. **å·¥å…·è°ƒç”¨é™åˆ¶**
å½“å‰å®ç°ä¸­ï¼Œå·¥å…·è°ƒç”¨åœ¨æµå¼æ¨¡å¼ä¸‹å¯èƒ½ä¸å®Œæ•´ï¼Œå»ºè®®åœ¨éœ€è¦å·¥å…·è°ƒç”¨æ—¶ä½¿ç”¨æ™®é€šæ¨¡å¼ã€‚

## ğŸš€ æ€§èƒ½ç‰¹ç‚¹

- **é¦–å­—èŠ‚æ—¶é—´çŸ­**: ç”¨æˆ·æ›´å¿«çœ‹åˆ°å“åº”
- **å†…å­˜å ç”¨ä½**: é€å—å¤„ç†ï¼Œä¸ç§¯ç´¯å®Œæ•´å“åº”
- **ç”¨æˆ·ä½“éªŒå¥½**: å®æ—¶åé¦ˆï¼Œå‡å°‘ç­‰å¾…ç„¦è™‘
- **é€‚åˆé•¿æ–‡æœ¬**: ç‰¹åˆ«é€‚åˆç”Ÿæˆé•¿ç¯‡å†…å®¹

---

**å®ç°æ—¥æœŸ**: 2025-10-27  
**ç‰ˆæœ¬**: v1.1.0  
**çŠ¶æ€**: âœ… å·²å®Œæˆå¹¶æµ‹è¯•
