"""è¯„ä¼°æœåŠ¡ - æµ‹è¯•ç»“æœè¯„ä¼°å’Œå·¥å…·è°ƒç”¨åŒ¹é…"""
import json
import logging
from typing import Dict, Any, List, Optional, Tuple
from difflib import SequenceMatcher

logger = logging.getLogger(__name__)


class EvaluationService:
    """æµ‹è¯•ç»“æœè¯„ä¼°æœåŠ¡"""
    
    @staticmethod
    def evaluate_result(
        output: str,
        expected_output: Optional[str],
        tool_calls: Optional[List[Dict[str, Any]]],
        expected_tool_calls: Optional[List[Dict[str, Any]]],
        evaluation_criteria: Optional[Dict[str, Any]] = None,
        evaluation_weights: Optional[Dict[str, int]] = None,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        tool_call_history: Optional[List[Dict[str, Any]]] = None
    ) -> Tuple[float, Dict[str, Any]]:
        """
        è¯„ä¼°æµ‹è¯•ç»“æœ
        
        Args:
            output: æ¨¡å‹è¾“å‡º
            expected_output: æœŸæœ›è¾“å‡º
            tool_calls: æ¨¡å‹çš„å·¥å…·è°ƒç”¨
            expected_tool_calls: æœŸæœ›çš„å·¥å…·è°ƒç”¨
            evaluation_criteria: è¯„ä¼°æ ‡å‡†
            evaluation_weights: è¯„åˆ†æƒé‡é…ç½® {tool_calls: 50, text_similarity: 20, tool_flow: 20, custom_criteria: 10}
            conversation_history: å¯¹è¯å†å²ï¼ˆç”¨äºæµç¨‹è¯„ä¼°ï¼‰
            tool_call_history: å·¥å…·è°ƒç”¨å†å²ï¼ˆç”¨äºæµç¨‹è¯„ä¼°ï¼‰
        
        Returns:
            (score, details) - åˆ†æ•°å’Œè¯¦ç»†ä¿¡æ¯
        """
        scores = {}
        details = {}
        
        # ä½¿ç”¨é»˜è®¤æƒé‡æˆ–ç”¨æˆ·è‡ªå®šä¹‰æƒé‡
        if not evaluation_weights:
            evaluation_weights = {
                'tool_calls': 50,
                'text_similarity': 20,
                'tool_flow': 20,
                'custom_criteria': 10
            }
        
        # 1. è¯„ä¼°å·¥å…·è°ƒç”¨ï¼ˆå¦‚æœæœ‰ï¼‰
        if expected_tool_calls:
            tool_score, tool_details = EvaluationService._evaluate_tool_calls(
                tool_calls, expected_tool_calls
            )
            scores['tool_call'] = tool_score
            details['tool_calls'] = tool_details
            logger.info(f"ğŸ“Š å·¥å…·è°ƒç”¨è¯„åˆ†: {tool_score:.2f}")
        
        # 2. è¯„ä¼°æ–‡æœ¬è¾“å‡ºï¼ˆå¦‚æœæœ‰æœŸæœ›è¾“å‡ºï¼‰
        if expected_output:
            text_score = EvaluationService._evaluate_text_similarity(
                output, expected_output
            )
            scores['text_similarity'] = text_score
            details['text_similarity'] = {
                'score': text_score,
                'output_length': len(output),
                'expected_length': len(expected_output)
            }
            logger.info(f"ğŸ“Š æ–‡æœ¬ç›¸ä¼¼åº¦: {text_score:.2f}")
        
        # 3. è¯„ä¼°å·¥å…·ä½¿ç”¨æµç¨‹ï¼ˆæ–°å¢ï¼‰
        if expected_tool_calls and (tool_call_history or conversation_history):
            flow_score, flow_details = EvaluationService.evaluate_tool_usage_flow(
                conversation_history=conversation_history,
                tool_call_history=tool_call_history,
                final_output=output,
                expected_tool_calls=expected_tool_calls
            )
            scores['tool_flow'] = flow_score
            details['tool_flow'] = flow_details
            logger.info(f"ğŸ“Š å·¥å…·ä½¿ç”¨æµç¨‹: {flow_score:.2f}")
        
        # 4. åº”ç”¨è‡ªå®šä¹‰è¯„ä¼°æ ‡å‡†
        if evaluation_criteria:
            custom_score = EvaluationService._apply_custom_criteria(
                output, tool_calls, evaluation_criteria
            )
            scores['custom'] = custom_score
            details['custom_criteria'] = evaluation_criteria
            logger.info(f"ğŸ“Š è‡ªå®šä¹‰è¯„ä¼°: {custom_score:.2f}")
        
        # è®¡ç®—æ€»åˆ†ï¼ˆä½¿ç”¨è‡ªå®šä¹‰æƒé‡æˆ–æ™ºèƒ½è°ƒæ•´ï¼‰
        if scores:
            # æ ¹æ®å®é™…è¯„ä¼°çš„ç»´åº¦æ™ºèƒ½è°ƒæ•´æƒé‡
            weights = {}
            total_weight = 0
            
            if 'tool_call' in scores:
                weights['tool_call'] = evaluation_weights.get('tool_calls', 50) / 100.0
                total_weight += evaluation_weights.get('tool_calls', 50)
            
            if 'text_similarity' in scores:
                weights['text_similarity'] = evaluation_weights.get('text_similarity', 20) / 100.0
                total_weight += evaluation_weights.get('text_similarity', 20)
            
            if 'tool_flow' in scores:
                weights['tool_flow'] = evaluation_weights.get('tool_flow', 20) / 100.0
                total_weight += evaluation_weights.get('tool_flow', 20)
            
            if 'custom' in scores:
                weights['custom'] = evaluation_weights.get('custom_criteria', 10) / 100.0
                total_weight += evaluation_weights.get('custom_criteria', 10)
            
            # å¦‚æœæ€»æƒé‡ä¸æ˜¯100%ï¼ˆæŸäº›ç»´åº¦ç¼ºå¤±ï¼‰ï¼Œåˆ™å½’ä¸€åŒ–æƒé‡
            if total_weight > 0 and total_weight != 100:
                normalization_factor = 100.0 / total_weight
                weights = {k: v * normalization_factor for k, v in weights.items()}
                logger.info(f"âš–ï¸  æƒé‡å½’ä¸€åŒ–: å®é™…æ€»æƒé‡ {total_weight}% -> å½’ä¸€åŒ–ä¸º 100%")
            
            total_score = sum(
                scores.get(key, 0) * weight 
                for key, weight in weights.items()
            )
            
            # ä¿å­˜å®é™…ä½¿ç”¨çš„æƒé‡åˆ°details
            details['weights_used'] = {k: round(v * 100, 2) for k, v in weights.items()}
        else:
            total_score = 0.0
        
        details['scores'] = scores
        details['total_score'] = total_score
        
        logger.info(f"âœ… æ€»è¯„åˆ†: {total_score:.2f}")
        
        return total_score, details
    
    @staticmethod
    def _evaluate_tool_calls(
        actual_calls: Optional[List[Dict[str, Any]]],
        expected_calls: List[Dict[str, Any]]
    ) -> Tuple[float, Dict[str, Any]]:
        """
        è¯„ä¼°å·¥å…·è°ƒç”¨çš„å‡†ç¡®æ€§
        
        è¯„åˆ†ç»´åº¦ï¼š
        1. åç§°åŒ¹é… (20åˆ†)
        2. æ•°é‡åŒ¹é… (20åˆ†)
        3. å‚æ•°åç§°åŒ¹é… (30åˆ†)
        4. å‚æ•°å€¼åŒ¹é… (30åˆ†)
        """
        # æ ‡å‡†åŒ–å·¥å…·è°ƒç”¨æ ¼å¼ä¸º {name, arguments}
        def normalize_tool_call(call):
            """å°†å·¥å…·è°ƒç”¨æ ‡å‡†åŒ–ä¸ºç»Ÿä¸€æ ¼å¼"""
            name = call.get('function', {}).get('name') or call.get('name')
            args = call.get('function', {}).get('arguments') or call.get('arguments', {})
            
            # å¦‚æœargumentsæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æ
            if isinstance(args, str):
                try:
                    args = json.loads(args)
                except:
                    args = {}
            
            return {
                'name': name,
                'arguments': args
            }
        
        # åˆå§‹åŒ–è¯¦æƒ…ï¼ŒåŒ…å«æ ‡å‡†åŒ–åçš„æœŸæœ›å’Œå®é™…çš„å·¥å…·è°ƒç”¨
        normalized_expected = [normalize_tool_call(call) for call in (expected_calls or [])]
        normalized_actual = [normalize_tool_call(call) for call in (actual_calls or [])]
        
        details = {
            'expected': normalized_expected,
            'actual': normalized_actual,
            'name_match': 0.0,
            'count_match': 0.0,
            'params_match': 0.0,
            'values_match': 0.0
        }
        
        # å¦‚æœæ²¡æœ‰æœŸæœ›çš„å·¥å…·è°ƒç”¨ï¼Œè¿”å›æ»¡åˆ†
        if not expected_calls:
            details['name_match'] = 20.0
            details['count_match'] = 20.0
            details['params_match'] = 30.0
            details['values_match'] = 30.0
            return 1.0, details
        
        # å¦‚æœæ²¡æœ‰å®é™…è°ƒç”¨å·¥å…·
        if not actual_calls:
            return 0.0, details
        
        # 1. åç§°åŒ¹é… (20åˆ†)
        expected_names = set(call['name'] for call in normalized_expected if call['name'])
        actual_names = set(call['name'] for call in normalized_actual if call['name'])
        
        if expected_names:
            name_match_ratio = len(expected_names & actual_names) / len(expected_names)
            details['name_match'] = name_match_ratio * 20.0
        else:
            details['name_match'] = 20.0
        
        # 2. æ•°é‡åŒ¹é… (20åˆ†)
        expected_count = len(normalized_expected)
        actual_count = len(normalized_actual)
        count_match_ratio = min(actual_count, expected_count) / expected_count if expected_count > 0 else 1.0
        details['count_match'] = count_match_ratio * 20.0
        
        # 3. å‚æ•°åç§°å’Œå€¼åŒ¹é… (60åˆ†)
        param_name_scores = []
        param_value_scores = []
        
        for expected in normalized_expected:
            expected_name = expected['name']
            expected_args = expected['arguments']
            
            # æŸ¥æ‰¾åŒ¹é…çš„å®é™…è°ƒç”¨
            best_param_name_score = 0.0
            best_param_value_score = 0.0
            
            for actual in normalized_actual:
                actual_name = actual['name']
                actual_args = actual['arguments']
                
                # åªæ¯”è¾ƒåŒåå·¥å…·
                if actual_name != expected_name:
                    continue
                
                # å‚æ•°åç§°åŒ¹é…åº¦
                if isinstance(expected_args, dict) and isinstance(actual_args, dict):
                    if expected_args:
                        # æ£€æŸ¥æ‰€æœ‰æœŸæœ›çš„å‚æ•°åç§°æ˜¯å¦éƒ½å­˜åœ¨
                        matching_keys = sum(1 for key in expected_args.keys() if key in actual_args)
                        param_name_score = matching_keys / len(expected_args)
                    else:
                        param_name_score = 1.0
                    
                    # å‚æ•°å€¼åŒ¹é…åº¦
                    param_value_score = EvaluationService._compare_parameters(
                        actual_args, expected_args
                    )
                    
                    if param_name_score > best_param_name_score:
                        best_param_name_score = param_name_score
                    if param_value_score > best_param_value_score:
                        best_param_value_score = param_value_score
            
            param_name_scores.append(best_param_name_score)
            param_value_scores.append(best_param_value_score)
        
        # è®¡ç®—å¹³å‡å‚æ•°åŒ¹é…åˆ†æ•°
        if param_name_scores:
            details['params_match'] = (sum(param_name_scores) / len(param_name_scores)) * 30.0
        else:
            details['params_match'] = 0.0
        
        if param_value_scores:
            details['values_match'] = (sum(param_value_scores) / len(param_value_scores)) * 30.0
        else:
            details['values_match'] = 0.0
        
        # è®¡ç®—æ€»åˆ† (0-1)
        total_score = (
            details['name_match'] +
            details['count_match'] +
            details['params_match'] +
            details['values_match']
        ) / 100.0
        
        return total_score, details
    
    @staticmethod
    def _compare_parameters(
        actual_params: Dict[str, Any],
        expected_params: Dict[str, Any]
    ) -> float:
        """
        æ¯”è¾ƒå‚æ•°çš„ç›¸ä¼¼åº¦
        
        Returns:
            0.0-1.0çš„ç›¸ä¼¼åº¦åˆ†æ•°
        """
        if not expected_params:
            return 1.0
        
        total_keys = len(expected_params)
        matched_keys = 0
        partial_matches = 0.0
        
        for key, expected_value in expected_params.items():
            if key not in actual_params:
                continue
            
            actual_value = actual_params[key]
            
            # ç²¾ç¡®åŒ¹é…
            if actual_value == expected_value:
                matched_keys += 1
            # ç±»å‹åŒ¹é…ä½†å€¼ä¸åŒ
            elif type(actual_value) == type(expected_value):
                if isinstance(expected_value, str):
                    # å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
                    similarity = SequenceMatcher(None, str(actual_value), str(expected_value)).ratio()
                    partial_matches += similarity * 0.5
                elif isinstance(expected_value, (int, float)):
                    # æ•°å€¼ç›¸ä¼¼åº¦
                    if expected_value != 0:
                        diff_ratio = abs(actual_value - expected_value) / abs(expected_value)
                        similarity = max(0, 1 - diff_ratio)
                        partial_matches += similarity * 0.5
        
        score = (matched_keys + partial_matches) / total_keys if total_keys > 0 else 0.0
        return min(1.0, score)
    
    @staticmethod
    def _evaluate_text_similarity(output: str, expected: str) -> float:
        """
        è¯„ä¼°æ–‡æœ¬ç›¸ä¼¼åº¦
        
        ä½¿ç”¨SequenceMatcherè®¡ç®—ç›¸ä¼¼åº¦
        """
        if not expected:
            return 1.0
        
        if not output:
            return 0.0
        
        similarity = SequenceMatcher(None, output.lower(), expected.lower()).ratio()
        return similarity
    
    @staticmethod
    def _apply_custom_criteria(
        output: str,
        tool_calls: Optional[List[Dict[str, Any]]],
        criteria: Dict[str, Any]
    ) -> float:
        """
        åº”ç”¨è‡ªå®šä¹‰è¯„ä¼°æ ‡å‡†
        
        æ”¯æŒçš„æ ‡å‡†ï¼š
        - min_length: æœ€å°è¾“å‡ºé•¿åº¦
        - max_length: æœ€å¤§è¾“å‡ºé•¿åº¦
        - must_contain: å¿…é¡»åŒ…å«çš„å…³é”®è¯
        - must_not_contain: ä¸èƒ½åŒ…å«çš„å…³é”®è¯
        - tool_must_call: å¿…é¡»è°ƒç”¨çš„å·¥å…·åç§°åˆ—è¡¨
        """
        score = 1.0
        penalties = []
        
        # é•¿åº¦æ£€æŸ¥
        if 'min_length' in criteria:
            min_len = criteria['min_length']
            if len(output) < min_len:
                penalty = 0.2
                score -= penalty
                penalties.append(f"è¾“å‡ºé•¿åº¦ä¸è¶³ (æœ€å°:{min_len}, å®é™…:{len(output)})")
        
        if 'max_length' in criteria:
            max_len = criteria['max_length']
            if len(output) > max_len:
                penalty = 0.1
                score -= penalty
                penalties.append(f"è¾“å‡ºé•¿åº¦è¶…å‡º (æœ€å¤§:{max_len}, å®é™…:{len(output)})")
        
        # å…³é”®è¯æ£€æŸ¥
        if 'must_contain' in criteria:
            keywords = criteria['must_contain']
            if isinstance(keywords, str):
                keywords = [keywords]
            for keyword in keywords:
                if keyword not in output:
                    penalty = 0.2 / len(keywords)
                    score -= penalty
                    penalties.append(f"ç¼ºå°‘å…³é”®è¯: {keyword}")
        
        if 'must_not_contain' in criteria:
            keywords = criteria['must_not_contain']
            if isinstance(keywords, str):
                keywords = [keywords]
            for keyword in keywords:
                if keyword in output:
                    penalty = 0.2 / len(keywords)
                    score -= penalty
                    penalties.append(f"åŒ…å«ç¦æ­¢å…³é”®è¯: {keyword}")
        
        # å·¥å…·è°ƒç”¨æ£€æŸ¥
        if 'tool_must_call' in criteria:
            required_tools = criteria['tool_must_call']
            if isinstance(required_tools, str):
                required_tools = [required_tools]
            
            called_tools = []
            if tool_calls:
                called_tools = [
                    call.get('function', {}).get('name') or call.get('name')
                    for call in tool_calls
                ]
            
            for tool_name in required_tools:
                if tool_name not in called_tools:
                    penalty = 0.3 / len(required_tools)
                    score -= penalty
                    penalties.append(f"æœªè°ƒç”¨å¿…éœ€å·¥å…·: {tool_name}")
        
        if penalties:
            logger.warning(f"è‡ªå®šä¹‰æ ‡å‡†æ£€æŸ¥å¤±è´¥: {', '.join(penalties)}")
        
        return max(0.0, score)
    
    @staticmethod
    def evaluate_tool_usage_flow(
        conversation_history: Optional[List[Dict[str, Any]]],
        tool_call_history: Optional[List[Dict[str, Any]]],
        final_output: str,
        expected_tool_calls: Optional[List[Dict[str, Any]]] = None
    ) -> Tuple[float, Dict[str, Any]]:
        """
        è¯„ä¼°å·¥å…·ä½¿ç”¨æµç¨‹çš„å®Œæ•´æ€§
        
        è¯„åˆ†ç»´åº¦ï¼š
        1. æ˜¯å¦æ‰§è¡Œäº†å·¥å…·è°ƒç”¨ (20åˆ†)
        2. å·¥å…·ç»“æœæ˜¯å¦åœ¨å¯¹è¯å†å²ä¸­ (20åˆ†)
        3. æœ€ç»ˆç­”æ¡ˆæ˜¯å¦åŸºäºå·¥å…·ç»“æœ (30åˆ†)
        4. å·¥å…·è°ƒç”¨é¡ºåºæ˜¯å¦åˆç† (30åˆ†)
        
        Args:
            conversation_history: å¯¹è¯å†å²ï¼ˆåŒ…å«toolæ¶ˆæ¯ï¼‰
            tool_call_history: å·¥å…·è°ƒç”¨å†å²
            final_output: æœ€ç»ˆè¾“å‡º
            expected_tool_calls: æœŸæœ›çš„å·¥å…·è°ƒç”¨
        
        Returns:
            (score, details) - åˆ†æ•°å’Œè¯¦ç»†ä¿¡æ¯
        """
        details = {
            "has_tool_execution": 0.0,
            "has_tool_results_in_history": 0.0,
            "final_answer_uses_tool_data": 0.0,
            "tool_sequence_reasonable": 0.0,
            "issues": []
        }
        
        # å¦‚æœæ²¡æœ‰æœŸæœ›å·¥å…·è°ƒç”¨ï¼Œè¿”å›æ»¡åˆ†
        if not expected_tool_calls:
            details["has_tool_execution"] = 20.0
            details["has_tool_results_in_history"] = 20.0
            details["final_answer_uses_tool_data"] = 30.0
            details["tool_sequence_reasonable"] = 30.0
            return 1.0, details
        
        # 1. æ˜¯å¦æ‰§è¡Œäº†å·¥å…·è°ƒç”¨
        if tool_call_history and len(tool_call_history) > 0:
            details["has_tool_execution"] = 20.0
        else:
            details["issues"].append("æœªæ‰§è¡Œä»»ä½•å·¥å…·è°ƒç”¨")
        
        # 2. å·¥å…·ç»“æœæ˜¯å¦åœ¨å¯¹è¯å†å²ä¸­
        if conversation_history:
            tool_messages = [msg for msg in conversation_history if msg.get("role") == "tool"]
            if tool_messages:
                details["has_tool_results_in_history"] = 20.0
            else:
                details["issues"].append("å¯¹è¯å†å²ä¸­ç¼ºå°‘å·¥å…·æ‰§è¡Œç»“æœ")
        
        # 3. æœ€ç»ˆç­”æ¡ˆæ˜¯å¦åŸºäºå·¥å…·ç»“æœ
        if tool_call_history and final_output:
            # æ£€æŸ¥å·¥å…·è¿”å›çš„å…³é”®æ•°æ®æ˜¯å¦å‡ºç°åœ¨æœ€ç»ˆè¾“å‡ºä¸­
            tool_data_found = False
            for tool_call in tool_call_history:
                tool_result = tool_call.get("result", {})
                # æå–å·¥å…·ç»“æœä¸­çš„å…³é”®å€¼
                if isinstance(tool_result, dict):
                    for key, value in tool_result.items():
                        if key not in ["success", "timestamp", "tool_name", "_mock_mode"]:
                            # æ£€æŸ¥å€¼æ˜¯å¦åœ¨æœ€ç»ˆè¾“å‡ºä¸­
                            value_str = str(value)
                            if len(value_str) > 3 and value_str in final_output:
                                tool_data_found = True
                                break
                if tool_data_found:
                    break
            
            if tool_data_found:
                details["final_answer_uses_tool_data"] = 30.0
            else:
                details["issues"].append("æœ€ç»ˆç­”æ¡ˆæœªä½¿ç”¨å·¥å…·è¿”å›çš„æ•°æ®")
                # ç»™éƒ¨åˆ†åˆ†ï¼ˆè‡³å°‘å°è¯•äº†å·¥å…·è°ƒç”¨ï¼‰
                details["final_answer_uses_tool_data"] = 10.0
        
        # 4. å·¥å…·è°ƒç”¨é¡ºåºæ˜¯å¦åˆç†
        if tool_call_history:
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„æ— æ„ä¹‰è°ƒç”¨
            tool_names = [tc.get("tool_name") for tc in tool_call_history]
            unique_tools = set(tool_names)
            
            # å¦‚æœå·¥å…·è°ƒç”¨æ•°é‡åˆç†ï¼ˆä¸è¶…è¿‡æœŸæœ›çš„2å€ï¼‰
            if len(tool_call_history) <= len(expected_tool_calls) * 2:
                details["tool_sequence_reasonable"] = 30.0
            else:
                details["issues"].append(f"å·¥å…·è°ƒç”¨æ¬¡æ•°è¿‡å¤š ({len(tool_call_history)} æ¬¡)")
                # ç»™éƒ¨åˆ†åˆ†
                details["tool_sequence_reasonable"] = 15.0
        else:
            details["issues"].append("æ— å·¥å…·è°ƒç”¨åºåˆ—")
        
        # è®¡ç®—æ€»åˆ†
        total_score = (
            details["has_tool_execution"] +
            details["has_tool_results_in_history"] +
            details["final_answer_uses_tool_data"] +
            details["tool_sequence_reasonable"]
        ) / 100.0
        
        logger.info(f"ğŸ” å·¥å…·ä½¿ç”¨æµç¨‹è¯„ä¼°: {total_score:.2f}")
        if details["issues"]:
            logger.warning(f"âš ï¸ å‘ç°é—®é¢˜: {', '.join(details['issues'])}")
        
        return total_score, details

