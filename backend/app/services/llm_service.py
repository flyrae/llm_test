"""LLMæœåŠ¡å±‚ - ç»Ÿä¸€çš„å¤§æ¨¡å‹è°ƒç”¨æ¥å£"""
import time
import asyncio
import logging
import json
from typing import Dict, Any, Optional, AsyncGenerator, List
import httpx
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic

from app.models.model_config import ModelConfigDB
from app.utils.validators import decrypt_api_key

# è·å–æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)


class LLMService:
    """å¤§æ¨¡å‹æœåŠ¡"""
    
    @staticmethod
    async def call_model(
        model_config: ModelConfigDB,
        content: list,
        system_prompt: Optional[str] = None,
        params: Optional[Dict[str, Any]] = None,
        stream: bool = False,
        tools: Optional[List[Dict[str, Any]]] = None,
        conversation_history: Optional[List[Dict[str, Any]]] = None
    ):
        """
        è°ƒç”¨å¤§æ¨¡å‹
        
        Args:
            model_config: æ¨¡å‹é…ç½®
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            params: æ¨¡å‹å‚æ•°ï¼ˆè¦†ç›–é»˜è®¤å‚æ•°ï¼‰
            stream: æ˜¯å¦æµå¼è¾“å‡º
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨ï¼ˆç”¨äºFunction Callingï¼‰
            conversation_history: å¤šè½®å¯¹è¯å†å² [{"role": "user/assistant", "content": "..."}]
        
        Returns:
            å¦‚æœstream=Trueï¼Œè¿”å›AsyncGenerator
            å¦‚æœstream=Falseï¼Œè¿”å›åŒ…å«outputã€metricsçš„å­—å…¸
        """
        # è®°å½•è¯·æ±‚å¼€å§‹
        logger.info(f"=" * 80)
        logger.info(f"ğŸš€ å¼€å§‹è°ƒç”¨å¤§æ¨¡å‹")
        logger.info(f"æ¨¡å‹é…ç½®: {model_config.name} ({model_config.provider}/{model_config.model_name})")
        logger.info(f"ç³»ç»Ÿæç¤ºè¯: {system_prompt[:100] if system_prompt else 'None'}{'...' if system_prompt and len(system_prompt) > 100 else ''}")
        # å¦‚æœæœ‰å¯¹è¯å†å²ï¼Œè®°å½•å†å²è½®æ•°
        if conversation_history:
            logger.info(f"ğŸ’¬ å¯¹è¯å†å²: {len(conversation_history)} æ¡æ¶ˆæ¯")
        logger.info(f"content: {json.dumps(content, ensure_ascii=False)[:100]}{'...' if len(json.dumps(content, ensure_ascii=False)) > 100 else ''}")
        logger.info(f"å‚æ•°: {params}")
        logger.info(f"æµå¼æ¨¡å¼: {stream}")
        if tools:
            tool_names = [t.get('function', {}).get('name', 'unknown') for t in tools]
            logger.info(f"å·¥å…·åˆ—è¡¨: {tool_names}")
        # åˆå¹¶å‚æ•°
        model_params = {**(model_config.default_params or {}), **(params or {})}
        # æ ¹æ®providerè°ƒç”¨ä¸åŒçš„æ¨¡å‹
        if model_config.provider == "openai":
            return await LLMService._call_openai(
                model_config, content, system_prompt, model_params, stream, tools, conversation_history
            )
        elif model_config.provider == "anthropic":
            return await LLMService._call_anthropic(
                model_config, content, system_prompt, model_params, stream, tools
            )
        elif model_config.provider == "local":
            return await LLMService._call_local(
                model_config, content, system_prompt, model_params, stream, tools
            )
        elif model_config.provider == "custom":
            return await LLMService._call_custom(
                model_config, content, system_prompt, model_params, stream, tools, conversation_history
            )
        else:
            raise ValueError(f"Unsupported provider: {model_config.provider}")
    
    @staticmethod
    async def _call_openai(
        model_config: ModelConfigDB,
        content: list,
        system_prompt: Optional[str],
        params: Dict[str, Any],
        stream: bool,
        tools: Optional[List[Dict[str, Any]]] = None,
        conversation_history: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """è°ƒç”¨OpenAI API"""
        logger.info(f"ğŸ“¡ è°ƒç”¨ OpenAI API: {model_config.api_endpoint or 'https://api.openai.com'}")
        
        api_key = decrypt_api_key(model_config.api_key) if model_config.api_key else None
        base_url = model_config.api_endpoint or None
        
        client = AsyncOpenAI(api_key=api_key, base_url=base_url)
        
        # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        if conversation_history:
            messages.extend(conversation_history)
            logger.info(f"ğŸ’¬ æ·»åŠ  {len(conversation_history)} æ¡å†å²æ¶ˆæ¯")
        # ç›´æ¥ç”¨contentæ•°ç»„
        messages.append({"role": "user", "content": content})
        
        start_time = time.time()
        
        try:
            # å‡†å¤‡è¯·æ±‚å‚æ•°
            request_params = {
                "model": model_config.model_name,
                "messages": messages,
                "temperature": params.get("temperature", 0.7),
                "top_p": params.get("top_p", 1.0),
                "max_tokens": params.get("max_tokens", 1000),
                "frequency_penalty": params.get("frequency_penalty", 0),
                "presence_penalty": params.get("presence_penalty", 0),
                "stream": stream
            }
            
            # å¦‚æœæä¾›äº†å·¥å…·å®šä¹‰ï¼Œæ·»åŠ åˆ°è¯·æ±‚ä¸­
            if tools:
                request_params["tools"] = tools
                request_params["tool_choice"] = "auto"
                logger.info(f"ğŸ”§ åŒ…å« {len(tools)} ä¸ªå·¥å…·å®šä¹‰")
            
            # è®°å½•è¯·æ±‚å‚æ•°ï¼ˆæ’é™¤messageså†…å®¹ä»¥é¿å…æ—¥å¿—è¿‡é•¿ï¼‰
            log_params = {k: v for k, v in request_params.items() if k != "messages"}
            log_params["messages"] = [{"role": msg["role"], "content": f"{msg['content'][:50]}..."} for msg in request_params["messages"]]
            logger.info(f"ğŸ“¤ å‘é€è¯·æ±‚å‚æ•°: {json.dumps(log_params, ensure_ascii=False, indent=2)}")
            
            # å¦‚æœæ˜¯æµå¼æ¨¡å¼ï¼Œè¿”å›å¼‚æ­¥ç”Ÿæˆå™¨
            if stream:
                return LLMService._stream_openai_response(
                    client, request_params, start_time, model_config
                )
            
            # éæµå¼æ¨¡å¼
            response = await client.chat.completions.create(**request_params)
            
            # è®¡ç®—å“åº”æ—¶é—´
            response_time = time.time() - start_time
            
            # è·å–å“åº”æ¶ˆæ¯
            message = response.choices[0].message
            output = message.content or ""
            logger.info(f"ğŸ”§ æ¨¡å‹è¿”å›ï¼š{message}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            tool_calls_info = None
            if hasattr(message, 'tool_calls') and message.tool_calls:
                tool_calls_info = []
                for tool_call in message.tool_calls:
                    tool_calls_info.append({
                        "id": tool_call.id,
                        "type": tool_call.type,
                        "function": {
                            "name": tool_call.function.name,
                            "arguments": tool_call.function.arguments
                        }
                    })
                logger.info(f"ğŸ”§ æ¨¡å‹è¯·æ±‚è°ƒç”¨ {len(tool_calls_info)} ä¸ªå·¥å…·:")
                for tc in tool_calls_info:
                    logger.info(f"   - {tc['function']['name']}: {tc['function']['arguments'][:100]}")
                
                # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ä½†æ²¡æœ‰æ–‡æœ¬è¾“å‡ºï¼Œç”Ÿæˆç®€çŸ­è¯´æ˜ï¼ˆä¸åŒ…å«è¯¦ç»†ä¿¡æ¯ï¼‰
                if not output:
                    output = f""
                    logger.info(f"ğŸ“ è®¾ç½®è¾“å‡ºæç¤º: {output}")
            
            # æ„å»ºæŒ‡æ ‡
            metrics = {
                "response_time": response_time,
                "prompt_tokens": response.usage.prompt_tokens,
                "completion_tokens": response.usage.completion_tokens,
                "total_tokens": response.usage.total_tokens,
                "estimated_cost": LLMService._estimate_openai_cost(
                    model_config.model_name,
                    response.usage.prompt_tokens,
                    response.usage.completion_tokens
                )
            }
            
            logger.info(f"âœ… OpenAI è°ƒç”¨æˆåŠŸ")
            logger.info(f"ğŸ“Š å“åº”æ—¶é—´: {response_time:.2f}s")
            logger.info(f"ğŸ“Š Tokens: {metrics['prompt_tokens']} + {metrics['completion_tokens']} = {metrics['total_tokens']}")
            logger.info(f"ğŸ’° ä¼°ç®—æˆæœ¬: ${metrics['estimated_cost']:.6f}")
            logger.info(f"ğŸ“ è¾“å‡ºé¢„è§ˆ: {output[:200]}{'...' if len(output) > 200 else ''}")
            
            result = {
                "output": output,
                "metrics": metrics,
                "status": "success"
            }
            
            # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
            if tool_calls_info:
                result["tool_calls"] = tool_calls_info
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ OpenAI è°ƒç”¨å¤±è´¥: {str(e)}")
            logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            return {
                "output": "",
                "metrics": {"response_time": time.time() - start_time},
                "status": "error",
                "error_message": str(e)
            }
    
    @staticmethod
    async def _stream_openai_response(
        client: AsyncOpenAI,
        request_params: Dict[str, Any],
        start_time: float,
        model_config: ModelConfigDB
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        å¤„ç†OpenAIæµå¼å“åº”
        
        Yields:
            åŒ…å«æµå¼æ•°æ®å—çš„å­—å…¸
        """
        logger.info(f"ğŸ”„ å¼€å§‹æµå¼è¾“å‡º")
        
        full_content = ""
        tool_calls_info = []
        prompt_tokens = 0
        completion_tokens = 0
        total_chunks = 0
        
        try:
            stream = await client.chat.completions.create(**request_params)
            
            async for chunk in stream:
                total_chunks += 1
                
                # å®‰å…¨åœ°è®¿é—® choices
                if not chunk.choices or len(chunk.choices) == 0:
                    continue
                
                delta = chunk.choices[0].delta
                
                # å¤„ç†æ–‡æœ¬å†…å®¹
                if hasattr(delta, 'content') and delta.content:
                    full_content += delta.content
                    yield {
                        "content": delta.content,
                        "done": False
                    }
                
                # å¤„ç†å·¥å…·è°ƒç”¨
                if hasattr(delta, 'tool_calls') and delta.tool_calls:
                    for tool_call in delta.tool_calls:
                        if hasattr(tool_call, 'function') and tool_call.function:
                            yield {
                                "tool_call": {
                                    "id": getattr(tool_call, 'id', None),
                                    "name": tool_call.function.name,
                                    "arguments": tool_call.function.arguments
                                },
                                "done": False
                            }
                
                # å¤„ç†usageä¿¡æ¯ï¼ˆé€šå¸¸åœ¨æœ€åä¸€ä¸ªchunkï¼‰
                # æ³¨æ„ï¼šä¸æ˜¯æ‰€æœ‰APIéƒ½ä¼šåœ¨æµå¼æ¨¡å¼è¿”å›usage
                if hasattr(chunk, 'usage') and chunk.usage:
                    usage = chunk.usage
                    if hasattr(usage, 'prompt_tokens'):
                        prompt_tokens = usage.prompt_tokens
                    if hasattr(usage, 'completion_tokens'):
                        completion_tokens = usage.completion_tokens
            
            response_time = time.time() - start_time
            
            # å¦‚æœæ²¡æœ‰è·å–åˆ°tokenä¿¡æ¯ï¼Œè¿›è¡Œä¼°ç®—
            if prompt_tokens == 0:
                # ç²—ç•¥ä¼°ç®—ï¼šè‹±æ–‡çº¦4å­—ç¬¦=1tokenï¼Œä¸­æ–‡çº¦1.5å­—ç¬¦=1token
                # è¿™é‡Œç®€å•æŒ‰2å­—ç¬¦=1tokenä¼°ç®—
                prompt_tokens = len(request_params.get("messages", [{}])[0].get("content", "")) // 2
            if completion_tokens == 0:
                completion_tokens = len(full_content) // 2
            
            # æ„å»ºæœ€ç»ˆå“åº”
            metrics = {
                "response_time": response_time,
                "prompt_tokens": prompt_tokens,
                "completion_tokens": completion_tokens,
                "total_tokens": prompt_tokens + completion_tokens,
                "estimated_cost": LLMService._estimate_openai_cost(
                    model_config.model_name,
                    prompt_tokens,
                    completion_tokens
                )
            }
            
            final_response = {
                "output": full_content,
                "metrics": metrics,
                "status": "success"
            }
            
            if tool_calls_info:
                final_response["tool_calls"] = tool_calls_info
            
            # å‘é€å®Œæˆä¿¡å·
            yield {
                "done": True,
                "final_response": final_response,
                "metrics": metrics
            }
            
            logger.info(f"âœ… æµå¼è¾“å‡ºå®Œæˆ")
            logger.info(f"ğŸ“Š å“åº”æ—¶é—´: {response_time:.2f}s")
            logger.info(f"ğŸ“Š æ€»å—æ•°: {total_chunks}")
            logger.info(f"ğŸ“Š Tokens: {prompt_tokens} + {completion_tokens} = {prompt_tokens + completion_tokens}")
            logger.info(f"ğŸ“ å®Œæ•´è¾“å‡ºé•¿åº¦: {len(full_content)} å­—ç¬¦")
            
        except Exception as e:
            logger.error(f"âŒ æµå¼è¾“å‡ºå¤±è´¥: {str(e)}")
            logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            import traceback
            logger.error(f"å †æ ˆè·Ÿè¸ª: {traceback.format_exc()}")
            yield {
                "done": True,
                "error": str(e),
                "status": "error"
            }
    
    @staticmethod
    async def _call_anthropic(
        model_config: ModelConfigDB,
        content: list,
        system_prompt: Optional[str],
        params: Dict[str, Any],
        stream: bool,
        tools: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """è°ƒç”¨Anthropic Claude API"""
        logger.info(f"ğŸ“¡ è°ƒç”¨ Anthropic API")
        
        api_key = decrypt_api_key(model_config.api_key) if model_config.api_key else None
        
        client = AsyncAnthropic(api_key=api_key)
        
        start_time = time.time()
        
        try:
            if tools:
                logger.info(f"ğŸ”§ åŒ…å« {len(tools)} ä¸ªå·¥å…·å®šä¹‰")
                
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": content})
            response = await client.messages.create(
                model=model_config.model_name,
                max_tokens=params.get("max_tokens", 1000),
                temperature=params.get("temperature", 0.7),
                system=system_prompt or "",
                messages=messages
            )
            
            response_time = time.time() - start_time
            output = response.content[0].text
            
            metrics = {
                "response_time": response_time,
                "prompt_tokens": response.usage.input_tokens,
                "completion_tokens": response.usage.output_tokens,
                "total_tokens": response.usage.input_tokens + response.usage.output_tokens,
                "estimated_cost": LLMService._estimate_anthropic_cost(
                    model_config.model_name,
                    response.usage.input_tokens,
                    response.usage.output_tokens
                )
            }
            
            logger.info(f"âœ… Anthropic è°ƒç”¨æˆåŠŸ")
            logger.info(f"ğŸ“Š å“åº”æ—¶é—´: {response_time:.2f}s")
            logger.info(f"ğŸ“Š Tokens: {metrics['prompt_tokens']} + {metrics['completion_tokens']} = {metrics['total_tokens']}")
            logger.info(f"ğŸ’° ä¼°ç®—æˆæœ¬: ${metrics['estimated_cost']:.6f}")
            logger.info(f"ğŸ“ è¾“å‡ºé¢„è§ˆ: {output[:200]}{'...' if len(output) > 200 else ''}")
            
            return {
                "output": output,
                "metrics": metrics,
                "status": "success"
            }
            
        except Exception as e:
            logger.error(f"âŒ Anthropic è°ƒç”¨å¤±è´¥: {str(e)}")
            logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            return {
                "output": "",
                "metrics": {"response_time": time.time() - start_time},
                "status": "error",
                "error_message": str(e)
            }
    
    @staticmethod
    async def _call_local(
        model_config: ModelConfigDB,
        content: list,
        system_prompt: Optional[str],
        params: Dict[str, Any],
        stream: bool,
        tools: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """è°ƒç”¨æœ¬åœ°æ¨¡å‹ï¼ˆOllamaç­‰ï¼‰"""
        endpoint = model_config.api_endpoint or "http://localhost:11434"
        logger.info(f"ğŸ“¡ è°ƒç”¨æœ¬åœ°æ¨¡å‹: {endpoint}")
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": content})
        
        start_time = time.time()
        
        try:
            if tools:
                logger.info(f"ğŸ”§ åŒ…å« {len(tools)} ä¸ªå·¥å…·å®šä¹‰ (æœ¬åœ°æ¨¡å‹å¯èƒ½ä¸æ”¯æŒ)")
                
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{endpoint}/api/chat",
                    json={
                        "model": model_config.model_name,
                        "messages": messages,
                        "stream": False,
                        "options": {
                            "temperature": params.get("temperature", 0.7),
                            "top_p": params.get("top_p", 1.0),
                        }
                    },
                    timeout=120.0
                )
                
                response_time = time.time() - start_time
                result = response.json()
                output = result.get("message", {}).get("content", "")
                
                metrics = {
                    "response_time": response_time,
                    "prompt_tokens": 0,  # Ollamaä¸æä¾›tokenè®¡æ•°
                    "completion_tokens": 0,
                    "total_tokens": 0,
                    "estimated_cost": 0.0
                }
                
                logger.info(f"âœ… æœ¬åœ°æ¨¡å‹è°ƒç”¨æˆåŠŸ")
                logger.info(f"ğŸ“Š å“åº”æ—¶é—´: {response_time:.2f}s")
                logger.info(f"ğŸ“ è¾“å‡ºé¢„è§ˆ: {output[:200]}{'...' if len(output) > 200 else ''}")
                
                return {
                    "output": output,
                    "metrics": metrics,
                    "status": "success"
                }
                
        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°æ¨¡å‹è°ƒç”¨å¤±è´¥: {str(e)}")
            logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            return {
                "output": "",
                "metrics": {"response_time": time.time() - start_time},
                "status": "error",
                "error_message": str(e)
            }
    
    @staticmethod
    async def _call_custom(
        model_config: ModelConfigDB,
        content: list,
        system_prompt: Optional[str],
        params: Dict[str, Any],
        stream: bool,
        tools: Optional[List[Dict[str, Any]]] = None,
        conversation_history: Optional[List[Dict[str, Any]]] = None
    ) -> Dict[str, Any]:
        """è°ƒç”¨è‡ªå®šä¹‰API"""
        # è‡ªå®šä¹‰APIçš„é€šç”¨å®ç°
        return await LLMService._call_openai(
            model_config, content, system_prompt, params, stream, tools, conversation_history
        )
    
    @staticmethod
    def _estimate_openai_cost(model_name: str, prompt_tokens: int, completion_tokens: int) -> float:
        """ä¼°ç®—OpenAI APIæˆæœ¬ï¼ˆUSDï¼‰"""
        # ç®€åŒ–çš„ä»·æ ¼è¡¨ï¼ˆ2024å¹´ä»·æ ¼ï¼‰
        prices = {
            "gpt-4": (0.03, 0.06),
            "gpt-4-turbo": (0.01, 0.03),
            "gpt-3.5-turbo": (0.0005, 0.0015),
        }
        
        for key, (input_price, output_price) in prices.items():
            if key in model_name.lower():
                return (prompt_tokens / 1000 * input_price) + (completion_tokens / 1000 * output_price)
        
        return 0.0
    
    @staticmethod
    def _estimate_anthropic_cost(model_name: str, input_tokens: int, output_tokens: int) -> float:
        """ä¼°ç®—Anthropic APIæˆæœ¬ï¼ˆUSDï¼‰"""
        prices = {
            "claude-3-opus": (0.015, 0.075),
            "claude-3-sonnet": (0.003, 0.015),
            "claude-3-haiku": (0.00025, 0.00125),
        }
        
        for key, (input_price, output_price) in prices.items():
            if key in model_name.lower():
                return (input_tokens / 1000 * input_price) + (output_tokens / 1000 * output_price)
        
        return 0.0
