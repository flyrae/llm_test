"""AgentæœåŠ¡ - æ”¯æŒå®Œæ•´çš„å·¥å…·è°ƒç”¨é—­ç¯"""
import json
import logging
from typing import Dict, Any, Optional, List, Tuple
from app.models.model_config import ModelConfigDB
from app.services.llm_service import LLMService
from app.services.mock_tool_executor import MockToolExecutor

logger = logging.getLogger(__name__)


class AgentService:
    """AgentæœåŠ¡ï¼Œæ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨å’Œæ¨ç†"""
    
    @staticmethod
    async def run_agent(
        model_config: ModelConfigDB,
        content: list,
        system_prompt: Optional[str] = None,
        params: Optional[Dict[str, Any]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        tools_config: Optional[Dict[str, Dict[str, Any]]] = None,
        conversation_history: Optional[List[Dict[str, Any]]] = None,
        use_mock: bool = False,
        max_iterations: int = 5
    ) -> Dict[str, Any]:
        """
        è¿è¡Œ Agentï¼Œæ”¯æŒå¤šè½®å·¥å…·è°ƒç”¨
        
        Args:
            model_config: æ¨¡å‹é…ç½®
            content: ç”¨æˆ·æ¶ˆæ¯å†…å®¹
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            params: æ¨¡å‹å‚æ•°
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨
            tools_config: å·¥å…·é…ç½®å­—å…¸ {tool_name: mock_config}
            conversation_history: å¯¹è¯å†å²
            use_mock: æ˜¯å¦ä½¿ç”¨æ¨¡æ‹Ÿå·¥å…·æ‰§è¡Œ
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
        
        Returns:
            åŒ…å«æœ€ç»ˆè¾“å‡ºã€æŒ‡æ ‡ã€å·¥å…·è°ƒç”¨å†å²ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        logger.info("=" * 80)
        logger.info("ğŸ¤– å¯åŠ¨ Agent è¿è¡Œ")
        logger.info(f"æ¨¡å‹: {model_config.name}")
        logger.info(f"Mockæ¨¡å¼: {use_mock}")
        logger.info(f"æœ€å¤§è¿­ä»£: {max_iterations}")
        
        # æ„å»ºæ¶ˆæ¯å†å²
        messages = []
        if conversation_history:
            messages.extend(conversation_history)
            logger.info(f"ğŸ’¬ åŠ è½½å¯¹è¯å†å²: {len(conversation_history)} æ¡æ¶ˆæ¯")
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        messages.append({
            "role": "user",
            "content": content
        })
        
        # è¿½è¸ªå·¥å…·è°ƒç”¨å†å²
        tool_call_history = []
        total_iterations = 0
        total_prompt_tokens = 0
        total_completion_tokens = 0
        total_cost = 0.0
        
        # è¿­ä»£æ‰§è¡Œ
        for iteration in range(max_iterations):
            total_iterations += 1
            logger.info(f"\n{'='*80}")
            logger.info(f"ğŸ”„ è¿­ä»£ {iteration + 1}/{max_iterations}")
            
            # è°ƒç”¨æ¨¡å‹
            result = await LLMService.call_model(
                model_config=model_config,
                content=messages[-1]["content"] if iteration == 0 else "",
                system_prompt=system_prompt if iteration == 0 else None,
                params=params,
                tools=tools,
                conversation_history=messages[:-1] if iteration == 0 else messages,
                stream=False
            )
            
            # ç´¯è®¡æŒ‡æ ‡
            metrics = result.get("metrics", {})
            total_prompt_tokens += metrics.get("prompt_tokens", 0)
            total_completion_tokens += metrics.get("completion_tokens", 0)
            total_cost += metrics.get("estimated_cost", 0)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
            tool_calls = result.get("tool_calls")
            
            if not tool_calls:
                # æ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œè¿”å›æœ€ç»ˆç»“æœ
                logger.info(f"âœ… Agent å®Œæˆï¼Œæ— éœ€å·¥å…·è°ƒç”¨")
                logger.info(f"ğŸ“ æœ€ç»ˆè¾“å‡º: {result.get('output', '')[:200]}...")
                
                return {
                    "output": result.get("output", ""),
                    "metrics": {
                        "total_iterations": total_iterations,
                        "total_prompt_tokens": total_prompt_tokens,
                        "total_completion_tokens": total_completion_tokens,
                        "total_tokens": total_prompt_tokens + total_completion_tokens,
                        "estimated_cost": total_cost,
                        "response_time": metrics.get("response_time", 0)
                    },
                    "status": result.get("status", "success"),
                    "tool_call_history": tool_call_history,
                    "conversation_history": messages,
                    "error_message": result.get("error_message")
                }
            
            # æœ‰å·¥å…·è°ƒç”¨ï¼Œè®°å½•å¹¶æ‰§è¡Œ
            logger.info(f"ğŸ”§ æ£€æµ‹åˆ° {len(tool_calls)} ä¸ªå·¥å…·è°ƒç”¨")
            
            # å°† assistant æ¶ˆæ¯æ·»åŠ åˆ°å†å²ï¼ˆåŒ…å«å·¥å…·è°ƒç”¨ï¼‰
            assistant_message = {
                "role": "assistant",
                "content": result.get("output") or "",
                "tool_calls": tool_calls
            }
            messages.append(assistant_message)
            
            # æ‰§è¡Œå·¥å…·è°ƒç”¨
            tool_results = []
            for tool_call in tool_calls:
                function_info = tool_call.get("function", {})
                tool_name = function_info.get("name")
                tool_call_id = tool_call.get("id")
                
                logger.info(f"  ğŸ”¨ æ‰§è¡Œå·¥å…·: {tool_name}")
                
                # è§£æå‚æ•°
                arguments_str = function_info.get("arguments", "{}")
                try:
                    arguments = json.loads(arguments_str) if isinstance(arguments_str, str) else arguments_str
                except json.JSONDecodeError:
                    logger.error(f"âŒ æ— æ³•è§£æå·¥å…·å‚æ•°: {arguments_str}")
                    arguments = {}
                
                # æ‰§è¡Œå·¥å…·ï¼ˆmock æˆ–çœŸå®ï¼‰
                if use_mock:
                    mock_config = tools_config.get(tool_name) if tools_config else None
                    tool_result = MockToolExecutor.execute_tool_call(
                        tool_name, arguments, mock_config
                    )
                else:
                    # TODO: å®ç°çœŸå®å·¥å…·æ‰§è¡Œ
                    tool_result = {
                        "success": False,
                        "error": "çœŸå®å·¥å…·æ‰§è¡Œå°šæœªå®ç°",
                        "note": "è¯·ä½¿ç”¨ use_mock=True è¿›è¡Œæµ‹è¯•"
                    }
                
                logger.info(f"  âœ… å·¥å…·æ‰§è¡Œå®Œæˆ: {json.dumps(tool_result, ensure_ascii=False)[:100]}")
                
                # è®°å½•å·¥å…·è°ƒç”¨å†å²
                tool_call_record = {
                    "iteration": iteration + 1,
                    "tool_name": tool_name,
                    "arguments": arguments,
                    "result": tool_result,
                    "tool_call_id": tool_call_id
                }
                tool_call_history.append(tool_call_record)
                tool_results.append(tool_call_record)
                
                # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°æ¶ˆæ¯å†å²
                tool_message = {
                    "role": "tool",
                    "tool_call_id": tool_call_id,
                    "content": json.dumps(tool_result, ensure_ascii=False)
                }
                messages.append(tool_message)
            
            logger.info(f"ğŸ“¦ å·²æ·»åŠ  {len(tool_results)} ä¸ªå·¥å…·ç»“æœåˆ°å¯¹è¯å†å²")
            
            # å¦‚æœè¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œç»ˆæ­¢
            if iteration == max_iterations - 1:
                logger.warning(f"âš ï¸ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}ï¼Œå¼ºåˆ¶ç»ˆæ­¢")
                return {
                    "output": "è¾¾åˆ°æœ€å¤§å·¥å…·è°ƒç”¨æ¬¡æ•°ï¼Œå¯èƒ½æœªå®Œæˆå…¨éƒ¨ä»»åŠ¡",
                    "metrics": {
                        "total_iterations": total_iterations,
                        "total_prompt_tokens": total_prompt_tokens,
                        "total_completion_tokens": total_completion_tokens,
                        "total_tokens": total_prompt_tokens + total_completion_tokens,
                        "estimated_cost": total_cost,
                        "response_time": metrics.get("response_time", 0)
                    },
                    "status": "max_iterations_reached",
                    "tool_call_history": tool_call_history,
                    "conversation_history": messages,
                    "warning": f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}"
                }
        
        # ä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œ
        return {
            "output": "",
            "metrics": {},
            "status": "error",
            "error_message": "Agent æ‰§è¡Œå¼‚å¸¸ç»ˆæ­¢"
        }
    
    @staticmethod
    def format_tool_call_history(tool_call_history: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–å·¥å…·è°ƒç”¨å†å²ä¸ºå¯è¯»æ–‡æœ¬"""
        if not tool_call_history:
            return "æ— å·¥å…·è°ƒç”¨"
        
        lines = []
        for record in tool_call_history:
            lines.append(f"\nè¿­ä»£ {record['iteration']}:")
            lines.append(f"  å·¥å…·: {record['tool_name']}")
            lines.append(f"  å‚æ•°: {json.dumps(record['arguments'], ensure_ascii=False)}")
            lines.append(f"  ç»“æœ: {json.dumps(record['result'], ensure_ascii=False)[:100]}...")
        
        return "\n".join(lines)
