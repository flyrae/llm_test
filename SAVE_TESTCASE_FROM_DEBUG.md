# 从调试页面保存测试用例功能说明

## 功能概述

在调试页面测试模型对话后，现在可以将整个对话过程（包括期望输出和期望工具调用）保存为测试用例，用于后续的自动化测试和评估。

## 新增功能

### 1. 快速填充期望结果

保存测试用例时，可以使用两个快捷按钮：

- **使用模型输出作为期望输出**：将模型的最后一次文本输出自动填充到"期望输出"字段
- **使用工具调用作为期望工具调用**：将模型请求的工具调用信息自动填充到"期望工具调用"字段（仅在模型有工具调用时显示）

### 2. 支持的期望结果类型

- **期望输出（文本内容）**：用于评估模型生成的文本内容是否符合预期
- **期望工具调用（JSON格式）**：用于评估模型是否正确地调用了指定的工具及参数

两者可以：
- 都不填写：仅保存对话记录，不进行自动评分
- 单独填写一个：只针对该方面进行评分
- 同时填写：综合评估文本输出和工具调用的准确性

## 使用步骤

1. 在调试页面完成对话测试
2. 点击"保存为测试用例"按钮
3. 在弹出的对话框中：
   - 填写测试用例标题（必填）
   - 填写分类和标签（可选）
   - 点击快捷按钮自动填充期望结果，或手动编辑
   - 查看预览信息，确认保存内容
4. 点击"确认保存"

## 保存的内容

测试用例将包含：

- **基本信息**：标题、分类、标签
- **对话内容**：
  - 系统提示词（如果有）
  - 对话历史（多轮对话的所有消息）
  - 当前提示词
- **期望结果**：
  - 期望输出（文本）
  - 期望工具调用（JSON）
- **关联工具**：选择的工具列表
- **元数据**：
  - 创建来源（调试页面）
  - 使用的模型
  - 参数配置
  - 实际输出
  - 实际工具调用
  - 响应时间和Token消耗

## 期望工具调用格式示例

```json
[
  {
    "id": "call_abc123",
    "type": "function",
    "function": {
      "name": "get_weather",
      "arguments": "{\"city\": \"北京\", \"unit\": \"celsius\"}"
    }
  }
]
```

或简化格式：

```json
[
  {
    "function": {
      "name": "get_weather",
      "arguments": {
        "city": "北京",
        "unit": "celsius"
      }
    }
  }
]
```

## 评分机制

在批量测试时，系统会根据保存的期望结果进行自动评分：

- **文本相似度**：比较实际输出与期望输出的相似程度
- **工具调用准确性**：检查是否调用了正确的工具及参数
- **综合评分**：根据评分权重配置计算总分

评分权重可以在测试用例中自定义配置。

## 典型应用场景

1. **Function Calling 测试**：测试模型是否能正确识别并调用指定工具
2. **对话能力测试**：测试模型在多轮对话中的表现
3. **回归测试**：保存已验证的对话案例，确保模型更新后表现一致
4. **A/B 测试**：对比不同模型或参数配置下的表现差异

## 注意事项

- 期望工具调用必须是有效的 JSON 格式
- 保存的测试用例可以在"测试用例"页面查看和编辑
- 元数据中保存了实际输出和工具调用，方便后续对比分析
