# 更新日志 - 调试页面保存测试用例功能增强

## 更新日期
2025-10-27

## 更新内容

### 功能增强：调试页面保存测试用例时支持期望工具调用

在调试页面保存测试用例时，现在可以：

1. **快速填充期望输出**
   - 新增按钮：一键将模型的最后输出填充为期望输出

2. **快速填充期望工具调用**
   - 新增按钮：一键将模型的工具调用信息填充为期望工具调用
   - 仅在模型有工具调用时显示

3. **同时支持两种期望结果**
   - 期望输出（文本内容）
   - 期望工具调用（JSON格式）
   - 可以单独或同时设置，用于不同的评估场景

4. **增强的预览信息**
   - 在保存前显示设置的期望输出和期望工具调用
   - 提供更清晰的保存内容概览

5. **元数据增强**
   - 保存实际的工具调用信息到元数据
   - 方便后续对比和分析

## 修改的文件

### 前端
- `frontend/pages/debug.html`
  - 新增期望工具调用输入框
  - 新增两个快捷填充按钮
  - 新增 `fillExpectedOutput()` 方法
  - 新增 `fillExpectedToolCalls()` 方法
  - 修改 `saveAsTestCase()` 方法，支持解析和保存期望工具调用
  - 更新表单数据结构，添加 `expected_tool_calls` 字段
  - 增强预览信息，显示期望结果的设置状态

### 后端
- `backend/app/api/testcases.py`
  - 修复创建测试用例时缺少 `conversation_history` 和 `evaluation_weights` 字段的问题

### 文档
- `SAVE_TESTCASE_FROM_DEBUG.md` - 新增功能说明文档
- `UPDATE_LOG_TESTCASE_EXPECTED.md` - 本更新日志

## 数据库模型

无需更改，以下字段已存在：
- `expected_tool_calls` - 期望的工具调用（JSON）
- `conversation_history` - 对话历史（JSON）
- `evaluation_weights` - 评分权重配置（JSON）

## 使用示例

### 场景1：测试Function Calling准确性

1. 在调试页面选择支持工具调用的模型
2. 选择需要测试的工具（如 `get_weather`）
3. 输入提示词："北京今天天气怎么样？"
4. 发送并查看模型是否正确调用工具
5. 点击"保存为测试用例"
6. 点击"使用工具调用作为期望工具调用"按钮
7. 保存

这样创建的测试用例会在批量测试时检查模型是否调用了正确的工具及参数。

### 场景2：同时测试文本输出和工具调用

1. 进行一次完整的对话测试
2. 模型既返回了文本内容，也调用了工具
3. 保存时同时点击两个快捷按钮
4. 保存后的测试用例会综合评估两个方面

### 场景3：多轮对话测试

1. 进行多轮对话测试
2. 在最后一轮保存为测试用例
3. 系统会保存完整的对话历史
4. 可以设置最后一轮的期望输出或工具调用

## 兼容性

- 向后兼容：不影响已有的测试用例
- 已有测试用例的 `expected_tool_calls` 字段为 `null`
- 可以随时编辑已有测试用例添加期望工具调用

## 后续优化建议

1. 在测试用例列表页面显示期望工具调用的信息
2. 在批量测试结果中详细展示工具调用的评分细节
3. 支持更灵活的工具调用匹配规则（如忽略某些参数）
4. 添加期望工具调用的可视化编辑器

## 测试建议

1. 测试保存包含工具调用的测试用例
2. 测试保存纯文本输出的测试用例
3. 测试同时保存期望输出和期望工具调用
4. 测试期望工具调用的JSON验证
5. 测试多轮对话的保存和恢复
